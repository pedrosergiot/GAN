from __future__ import division, print_function

from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout
from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, UpSampling2D, Conv2D, LeakyReLU
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam

import matplotlib.pyplot as plt

import sys

import numpy as np

class GAN():
    def __init__(self):
        self.img_rows = 28
        self.img_cols = 28
        self.channels = 1   #imagens passadas em grayscale com apenas um canal
        self.img_shape = (self.img_rows, self.img_cols, self.channels)
        self.latent_dim = 100   #dimensao do espaco latente (ruido)
        
        optimizer = Adam(0.0002, 0.5)   # taxa de aprendizado e beta1 do otimizador Adam
        
        # Criando e compilando o discriminador
        self.discriminator = self.build_discriminator()
        self.discriminator.compile(loss = 'binary_crossentropy',
                                  optimizer = optimizer,
                                  metrics = ['accuracy'])
        
        # Construindo e compilando o gerador
        self.generator = self.build_generator()
        
        # Gerador recebe ruido na entrada e gera imagens
        z = Input(shape=(self.latent_dim))
        img = self.generator(z)
        
        # Somente o gerador e treinado no modelo da GAN
        self.discriminator.trainable = False
        
        # O discriminador recebe imagens geradas e determina sua validade
        validity = self.discriminator(img)
        
        # O modelo combinado treina o gerador com base na resposta do discriminador
        self.combined = Model(z, validity)
        self.combined.compile(loss = 'binary_crossentropy',
                             optimizer = optimizer)
        
        
    def build_generator(self):
        
        model = Sequential()
        
        model.add(Dense(256, input_dim=self.latent_dim))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(1024))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(np.prod(self.img_shape), activation='tanh'))
        model.add(Reshape(self.img_shape))
        
        model.summary()
        
        noise = Input(shape=(self.latent_dim,))
        img = model(noise)
        
        return Model(noise, img)
    
    def build_discriminator(self):
        
        model = Sequential()
        
        model.add(Flatten(input_shape=self.img_shape))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dense(256))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dense(1, activation='sigmoid'))
        
        model.summary()
        
        img = Input(shape=self.img_shape)
        validity = model(img)
        
        return Model(img, validity)
    
    
    def train(self, epochs, batch_size=128, sample_interval=50):
        
        (X_train, _),(_, _) = mnist.load_data()
        
        # Reescalando dados de [0 255] (greyscale) para [-1 1]
        X_train = (X_train - 127.5)/127.5
        X_train = np.expand_dims(X_train, axis=3)  #aumento de dimensao para casar com camadas convolucionais do modelo
        
        # Labels para falsos e verdadeiros
        valid = np.ones((batch_size, 1))
        fake = np.zeros((batch_size, 1))
        
        for epoch in range(epochs):
            
            # Treinamento do Discriminador
            
            # Seleciona um conjunto aleatorio de imagens
            idx = np.random.randint(0, X_train.shape[0], batch_size)
            imgs = X_train[idx]
            
            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
            
            # Gera um conjunto de novas imagens
            gen_imgs = self.generator.predict(noise)
            
            # Treina o discriminador
            d_loss_real = self.discriminator.train_on_batch(imgs, valid)
            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)
            d_loss = 0.5*np.add(d_loss_fake, d_loss_real)
            
            # Treina Gerador
            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
            
            # Treina Gerador (usa indices como se dados criados fossem verdadeiros para induzir erro no discriminador)
            g_loss = self.combined.train_on_batch(noise, valid)
            
            # Plot do progresso do treinamento
            print("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss))
            
            # Salvando imagens geradas com certo intervalo de epocas
            if epoch % sample_interval == 0:
                self.sample_images(epoch)
                self.save_model()

            
    def sample_images(self, epoch):
        r, c = 10, 10
        noise = np.random.normal(0, 1, (r*c, self.latent_dim))
        gen_imgs = self.generator.predict(noise)
        
        # Reescalando imagens para [0 1]
        gen_imgs = 0.5*gen_imgs + 0.5
        
        fig, axs = plt.subplots(r, c)
        cnt = 0
        for i in range(r):
            for j in range(c):
                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')
                axs[i,j].axis('off')
                cnt+=1
        fig.savefig("images_gan_mlp/%d.png" % epoch)
        plt.close()
        
        
    def save_model(self):

        def save(model, model_name):
            model_path = "saved_model/%s.json" % model_name
            weights_path = "saved_model/%s_weights.hdf5" % model_name
            options = {"file_arch": model_path,
                        "file_weight": weights_path}
            json_string = model.to_json()
            open(options['file_arch'], 'w').write(json_string)
            model.save_weights(options['file_weight'])

        save(self.generator, "generator")
        save(self.discriminator, "discriminator")
        
if __name__ == '__main__':
    gan = GAN()
    gan.train(epochs=14000, batch_size=32, sample_interval=200)
