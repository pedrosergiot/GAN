{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gan_mlp_cifar10_2.ipynb","provenance":[{"file_id":"19sKi7vkBgx56NN7W5XgnPMC5I-Hhhmth","timestamp":1569266191601}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"qDdVeLOZ8gAJ","colab_type":"code","outputId":"2f6bdf2c-6eb2-44de-f87d-04118b7558a4","executionInfo":{"status":"ok","timestamp":1569292867787,"user_tz":180,"elapsed":3316045,"user":{"displayName":"Pedro SÃ©rgio","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDNOQ8qokex1JEcP530XFUpBefCchi0RJaiVj_ahw=s64","userId":"09920799442367656805"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"12l25TgnQiCP06ErJgwbvPJDMiTDvxcS_"}},"source":["from __future__ import division, print_function\n","\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n","from tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.initializers import RandomNormal\n","\n","import matplotlib.pyplot as plt\n","\n","import sys\n","\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","\n","class GAN():\n","    def __init__(self):\n","        self.img_rows = 32\n","        self.img_cols = 32\n","        self.channels = 3   #imagens passadas em grayscale com apenas um canal\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","        self.latent_dim = 100   #dimensao do espaco latente (ruido)\n","        \n","        optimizer = Adam(0.0002, 0.5)   # taxa de aprendizado e beta1 do otimizador Adam\n","        \n","        # Criando e compilando o discriminador\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(loss = ['binary_crossentropy'], \n","                                  optimizer = optimizer,\n","                                  metrics = ['accuracy'])\n","        \n","        # Construindo e compilando o gerador\n","        self.generator = self.build_generator()\n","        \n","        # Gerador recebe ruido na entrada e gera imagens\n","        z = Input(shape=(self.latent_dim))\n","        img = self.generator(z)\n","        \n","        # Somente o gerador e treinado no modelo da GAN\n","        self.discriminator.trainable = False\n","        \n","        # O discriminador recebe imagens geradas e determina sua validade\n","        validity = self.discriminator(img)\n","        \n","        # O modelo combinado treina o gerador com base na resposta do discriminador\n","        self.combined = Model(z, validity)\n","        self.combined.compile(loss = 'binary_crossentropy',\n","                             optimizer = optimizer)\n","        \n","        \n","    def build_generator(self):\n","      \n","        init=RandomNormal(stddev=0.02)\n","        \n","        model = Sequential()\n","        \n","        model.add(Dense(256, input_dim=self.latent_dim, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization())\n","        model.add(Dense(512, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization())\n","        model.add(Dense(1024, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization())\n","        model.add(Dense(2048, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization())\n","        model.add(Dense(np.prod(self.img_shape), activation='tanh', kernel_initializer=init))\n","        model.add(Reshape(self.img_shape))\n","        \n","        model.summary()\n","        \n","        noise = Input(shape=(self.latent_dim,))\n","        img = model(noise)\n","        \n","        return Model(noise, img)\n","    \n","    \n","    def build_discriminator(self):\n","        \n","        init=RandomNormal(stddev=0.02)\n","\n","        model = Sequential()\n","        \n","        model.add(Flatten(input_shape=self.img_shape))\n","        model.add(Dense(1024, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dense(512, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dense(256, kernel_initializer=init))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dense(1, activation='sigmoid'))\n","        \n","        model.summary()\n","                \n","        img = Input(shape=self.img_shape)        \n","        validity = model(img)\n","        \n","        return Model(img, validity)\n","    \n","    \n","    def train(self, epochs, batch_size=128, sample_interval=50):\n","        \n","        (X_train, _),(_, _) = cifar10.load_data()\n","        \n","        # Reescalando dados de [0 255] (greyscale) para [-1 1]\n","        X_train = (X_train - 127.5)/127.5\n","        \n","        # Labels para falsos e verdadeiros\n","        valid = np.ones((batch_size, 1))\n","        fake = np.zeros((batch_size, 1))\n","        \n","        D_loss = []\n","        G_loss = []\n","        Acc = []\n","        \n","        for epoch in range(epochs):\n","            \n","            # Treinamento do Discriminador\n","            \n","            # Seleciona um conjunto aleatorio de imagens\n","            idx = np.random.randint(0, X_train.shape[0], batch_size)\n","            imgs = X_train[idx]\n","            \n","            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","            \n","            # Gera um conjunto de novas imagens\n","            gen_imgs = self.generator.predict(noise)\n","            \n","            # Treina o discriminador\n","            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","            d_loss = 0.5*np.add(d_loss_fake, d_loss_real)\n","            \n","            # Treina Gerador\n","            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","            \n","            # Treina Gerador (usa indices como se dados criados fossem verdadeiros para induzir erro no discriminador)\n","            g_loss = self.combined.train_on_batch(noise, valid)\n","            \n","            # Plot do progresso do treinamento\n","            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","            \n","            D_loss.append(d_loss[0])\n","            G_loss.append(g_loss)\n","            Acc.append(d_loss[1])\n","            \n","            # Salvando imagens geradas com certo intervalo de epocas\n","            if epoch % sample_interval == 0:\n","                self.sample_images(epoch)\n","                self.save_model()\n","                \n","        return D_loss, G_loss, Acc\n","\n","            \n","    def sample_images(self, epoch):\n","        r, c = 5, 5\n","        noise = np.random.normal(0, 1, (r*c, self.latent_dim))\n","        gen_imgs = self.generator.predict(noise)\n","        \n","        # Reescalando imagens para [0 1]\n","        gen_imgs = 0.5*gen_imgs + 0.5\n","        \n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt,:,:,:])\n","                axs[i,j].axis('off')\n","                cnt+=1\n","        fig.savefig(\"/content/gdrive/My Drive/Deep Learning/gan_mlp_cifar10_2/images/%d.png\" % epoch)\n","        plt.close()\n","        \n","        \n","    def save_model(self):\n","\n","        def save(model, model_name):\n","            model_path = \"/content/gdrive/My Drive/Deep Learning/gan_mlp_cifar10_2/saved_model/%s.json\" % model_name\n","            weights_path = \"/content/gdrive/My Drive/Deep Learning/gan_mlp_cifar10_2/saved_model/%s_weights.hdf5\" % model_name\n","            options = {\"file_arch\": model_path,\n","                        \"file_weight\": weights_path}\n","            json_string = model.to_json()\n","            open(options['file_arch'], 'w').write(json_string)\n","            model.save_weights(options['file_weight'])\n","\n","        save(self.generator, \"generator\")\n","        save(self.discriminator, \"discriminator\")\n","        \n","if __name__ == '__main__':\n","    gan = GAN()\n","    D_loss, G_loss, Acc = gan.train(epochs=50000, batch_size=100, sample_interval=200)"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}
